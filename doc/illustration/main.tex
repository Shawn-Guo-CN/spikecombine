\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{times}
\usepackage{fancyhdr,graphicx,amsmath,amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[colorlinks,linkcolor=blue]{hyperref}

\include{pythonlisting}

\title{Illustration for Combining Sorting Results from Multiple Sorters}
\author{Shangmin Guo}
\date{\today}

\begin{document}
\maketitle

In this document, I would give a brief illustration of the current derivation and implementation of how I combine different sorting results from different sorters. The Github repo of this algorithm is \href{https://github.com/Shawn-Guo-CN/spikecombine}{https://github.com/Shawn-Guo-CN/spikecombine}.

\section{Derivation of the Algorithm}
\label{sec:derivation}

\subsection{Preliminary}
\label{ssec:preliminary}

Assume that we first have the following things:

\begin{itemize}
    \item $K$ sorters, i.e. $\left\{ s_1, s_2, \dots, s_K \right\}$;
    \item $N^{(k)}$ units from with their metric values 
    $\mathbf{m}^{(k)}_i=\left\{ m^{(k)}_{i,1}, m^{(k)}_{i,2}, \dots, m^{(k)}_{i,D} \right\},\ \forall i \in \{1, 2, \dots, N^{(k)}\}$
    for every sorting $s_k$, where $D$ is the number of metrics we have.
\end{itemize}

Then, we task is: for a specific sorter $s_k$, decide whether unit $u^{(k)}_i$ it detected is a true positive (TP) unit (i.e.  $u^{(k)}_i=1$) or false positive (FP) one (i.e. $u^{(k)}_i=0$), based on their corresponding metric values. As we would only curate the sortings from one sorter, from then on, I will ignore the serial number $(k)$ of different sorters.

\subsection{Derivation}
\label{ssec:derivation}

Based on the Bayesian theorem and the task, I could derive the posterior probability of some unit $u_i$ being a TP as follow:

\begin{equation}
    \begin{split}
        p(u_i=1|\mathbf{m}_i) 
            & = \sum_{k=1}^{K} p(u_i=1, s_k | \mathbf{m}_i) \\
            & = \sum_{k=1}^{K} \frac{p(u_i=1, s_k, \mathbf{m}_i)}{\sum_{k'=1}^{K}\sum_{i'=1}^{N^{(k')}}p(u_{i'}, s_{k'}, \mathbf{m}_{i'})} \\
            & \propto p(u_i=1, s_k, \mathbf{m}_i) \\
            & = p(\mathbf{m}_i | u_i=1, s_k) p(u_i=1|s_k) p(s_k)
    \end{split} \label{eq:postorior_prob_TP}
\end{equation}

Similarly, we could have:

\begin{equation}
    p(u_i=0|\mathbf{m}_i)  \propto p(\mathbf{m}_i|u_i=0, s_k) p(u_i=0|s_k) p(s_k) \label{eq:postorior_prob_FP}
\end{equation}

\textbf{Note}: $p(u_i|s_k)$ is a delta distribution whose value is decided by \textit{ spikecomparison.compare\_two\_sorters() }.
So, it doesn't have a probabilistic explanation.

Then, take $p(u_i=1|\mathbf{m}_i)$ for example, what we need to estimate is only $p(\mathbf{m}_i | u_i=1, s_k)$, i.e. the posterior distribution of metric values on TP units from sorter $s_k$.
As for the prior distribution $p(s_k)$, we can set it to uniform.
And $p(u_i=1|s_k)$ could be estimated by: i) the agreement score between $s_k$ and ground-truth during learning; ii) the agreement score between $s_k$ and other sorter $s_{k'}$ during inference. 

\section{Implementation of the Algorithm}
\label{sec:implementation}

\subsection{Hierarchy of Classes}
\label{ssec:hierarchy}

In the project, all the classes and their brief descriptions are given as follows:

\begin{itemize}
    \item \textbf{SpikeSortersCombiner}, the class implementing the algorithm with a function \textit{fit} for fitting $p(\mathbf{m}_i | u_i=1, s_k)$ and a function \textit{predict} for performing the inference in Equation(1) and Equation (2).
    \item \textbf{DataGenerator}, the class is to load a folder containing lots of \textit{MEArecRecordingExtractor} in .h format, and (more importantly) generate a folder that is specifically organised to be taken as the input for \textit{DataLoader}.
    \item \textbf{DataLoader}, the class is to take the folder generated by \textit{DataGenerator} as the input, and provide \textit{SpikeSortersCombiner} APIs to get access the \textit{sortings} from different sorters with the corresponding \textit{ground-truth sorting} and \textit{original recording}.
\end{itemize}

\subsection{Descriptions of Key Steps}
\label{ssec:descriptions}

Here, I did not provide pseudocodes for the key steps, as the codes themselves are all very straightforward.

\subsubsection{For Fitting}
\label{ssec:fit_pseudocode}

The steps of fitting parameters of $p(\mathbf{m}_i | u_i=1, s_k)$ of \textbf{every sorter} $s_k$ are given as follows:

\begin{enumerate}
    \item by comparing with the corresponding ground-truth sorting, classify the units to TP $U_k^{TP}$ ones and FP ones $U_k^{FP}$ for every sorter $s_k$, based on the agreement scores between $s_k$ and ground-truth;
    \item estimate the mean and covariance matrix on: i) $U_k^{TP}$, which gives $\mathbf{\mu}_k^{TP}$ and  $\mathbf{\sigma}_k^{TP}$; ii)  $U_k^{FP}$, which gives $\mathbf{\mu}_k^{FP}$ and  $\mathbf{\sigma}_k^{FP}$.
\end{enumerate}

\subsubsection{For Inference}
\label{ssec:fit_inference}

The steps to do inference on sortings from $s_k$ are exactly following Equation (1) and (2):

\begin{enumerate}
    \item compare $s_k$ with sortings from all other different sorters, and store the agreement scores between them;
    \item calculate the metric $\mathbf{m}_i^{(k)}$ for every detected unit $u_i^{(k)}$;
    \item for every $u_i^{(k)}$, calculate $p(u_i=1|\mathbf{m}_i)$ and $p(u_i=1|\mathbf{m}_i)$ following Equation (1) and (2) respectively;
    \item if $p(u_i=1|\mathbf{m}_i) < p(u_i=1|\mathbf{m}_i)$, then exclude $u_i^{(k)}$ from the output of $s_k$.
\end{enumerate}

\end{document}